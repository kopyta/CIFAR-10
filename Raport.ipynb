{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPr8DNzic5zL"
   },
   "source": [
    "# Enhancing Performance on CIFAR-10 using ResNet-18 models\n",
    "#### Agata Kopyt\n",
    "\n",
    "### Introduction\n",
    "The objective of the term project is to improve the performance on the CIFAR-10 dataset using the ResNet-18 architecture. The CIFAR-10 dataset consists of 60,000 32x32 color images across 10 different classes. The goal is to develop a model that classifies these images with the best accuracy. The dataset can be downloaded from the [website](https://www.cs.toronto.edu/~kriz/cifar.html).The baseline code to which improvements were added is availabale in the [linked repository](https://github.com/heechul-knu/cifar-baseline). My means to create the best deep learning model were limited due to not being in possession of device with GPU and limitations of Google Colab, nevertheless I did my best.\n",
    "\n",
    "### Architecture\n",
    "ResNet-18, short for Residual Network-18, is a popular deep convolutional neural network (CNN) architecture that has significantly advanced the field of computer vision. It consists of 18 layers with residual connections, which allow a direct flow of information across layers. Therefore, it effectively mitigates the vanishing gradient problem. Its outstanding performance on various image classification tasks has made ResNet-18 a famous architecture among researchers and practitioners. For the term project only the ResNet-18 architecture without any modifications was allowed.\n",
    "\n",
    "### Methodology\n",
    "For the methodology I started with implementing data transformation and augmentation practices learned at the Deep Learning course by myself. Additionally, I experimented with the optimizer choice. Later I researched about the best practices for improving CIFAR-10 accuracy, already discovered by the data scientists.\n",
    "\n",
    "#### Optimizer\n",
    "I started improvements with choosing the optimizer. The baseline code used famously recommended SGDM optimizer. I decided to compare the performances of the models using Adam optimizer and Adamax optimizer.\n",
    "\n",
    "#### Data Augmentation and Transformation\n",
    "The baseline code had already implemented resizing, random cropping, random horizontal flip and normalization. Based on the knowledge from the Deep Learning course, I additionally performed on the training dataset random vertical flip, random rotation, gaussian blur, color jitter, random affine and random perspective.\n",
    "\n",
    "In the [PyTorch documentation](https://pytorch.org/vision/stable/transforms.html#auto-augmentation) I discovered Auto-Augmentation functions. Among the offered functions, `AutoAugment` function was based on the article [\"AutoAugment:\n",
    "Learning Augmentation Strategies from Data\"](https://arxiv.org/pdf/1805.09501.pdf) written by Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan and Quoc V. Le. The procedure introduced in the article was developed using Res-net architecture and CIFAR-10 dataset (among others) and offers a policy learned on a CIFAR-10 dataset. It improved the baseline accuracy  to **97.17%** (using ensembled 2 models created using 50 epochs each). The accuracy of 5 models (using 60 epochs for each) ensembled improved to **97.4%**.\n",
    "\n",
    "Inspired by the paper [\"Random Erasing Data Augmentation\"](https://ojs.aaai.org/index.php/AAAI/article/view/7000) written by Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li and Yi Yang, I performed random erasing on the training dataset. The article states that this method improves the robustness of CNNs to partially occluded\n",
    "samples, in particullar using CIFAR-10 dataset and ResNet architectures.\n",
    "\n",
    "####  Improvement of Model Development\n",
    " To improve the baseline models, I experimented with number of models to ensemble and the number of epochs fro each model. Also, I aspired to implement information learned in the article called [\"Shake-Shake regularization\"](https://www.cs.toronto.edu/~kriz/cifar.html) by Xavier Gastaldi. The idea introduced in this paper helps to solve the overfitting problem. The researcher himself used CIFAR-10 to present the promising performance of the method, giving high hopes for improvement. Unfortunately, before I implemented it in my code, I ran out of computing units on Google Colab and was forced to abandon this idea.\n",
    "\n",
    "#### Performance Evaluation: \n",
    "To evaluate the performances of created models I used the accuracy.\n",
    "\n",
    "### Results\n",
    "Firstly I compared the results of SGDM, Adam and Adamax optimizers. Adamax optimizer turned out to provide the best accuracy and I chosed it for developing the final models.\n",
    "\n",
    "Then I experimented with data transformation and augmentation methods. To minimize the time of compiling the code during constant development of the model, I tested those methods one by one on models created on small number of epochs (20). Only resizing, random cropping, random horizontal flip and normalization improved the accuracy, while the other methods worsen it. Keeping in mind that sometimes transformation or augmentation methods can initially provide noise and worsen the accuracy, because of low number of epochs used for the models, I checked the performance of the models created with mentioned methods and higher number of epochs (60). Unfortunately, the accuracy did not improve.\n",
    "\n",
    "Using `AutoAugment` with CIFAR-10 policy improved the baseline accuracy  to **97.17%** (using ensembled 2 models created using 50 epochs each). The accuracy of 5 models ensembled (using 60 epochs for each) improved to **97.4%**.\n",
    "\n",
    "Finally I added random erasing to the training dataset. Ensembling 3 models with 55 epochs for each increased the accuracy to **97,43%**. I tried training and esembling 5 such models, but the accuracy dropped to **97,38%**. By comparing the accuracies I found the best combination from the 5 models which resulted in the best being esembling the first 3 models.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Among SGDM, Adam and Adamax, Adamax is the best optimizer for the problem introduced in the term project.\n",
    "\n",
    "Testing and combining various transformation and augmentation methods for enhencing the accuracy of classification model resulted in `AutoAugment` and random erasing being the best, in additon to the methods used in the baseline model. That is reasonable result, beacuse `AutoAugment` was created by the group of researchers with experience and better resources, especially to be used on CIFAR-10 dataset. Other methods, which failed, could supposedly improve the performance with even larger number of epochs, but the resources limited that aspirations.\n",
    "\n",
    "Eventhough ensembling larger number of models (5) with auto-augmentation improved the accuracy, when random ereasing was added, smaller number of models (3) turned out to be sufficient and even performed better than ensembling 4 or 5 models.\n",
    "\n",
    "Increasing number of epochs at some point was not efficient due to small improvements over long compiling time but, using a better device, might provide even better accuracy.\n",
    "\n",
    "Implementing Shake-Shake regularization could result in better performance.\n",
    "\n",
    "Overall the result of **97,43%** is satisfactory improvement given the limited resources. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPHRpuGfFD0JyY9CYC8XxD1",
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
